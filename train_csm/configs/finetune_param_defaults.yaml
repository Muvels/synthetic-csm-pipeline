# Sesame CSM Fine-tuning Configuration
# Default parameters for training

# Dataset settings
num_context_turns: 3         # Number of previous turns to include as context
max_audio_seconds: 15.0      # Maximum audio length in seconds
context_length: 2048         # Max context length in tokens

# Training hyperparameters
batch_size: 8
grad_acc_steps: 1
learning_rate: 0.00003       # 3e-5
warmup_steps: 1000
weight_decay: 0.002
lr_decay: linear
max_grad_norm: 1.3

# LoRA settings (when not doing full finetune)
lora_r: 32
lora_alpha: 32

# Checkpointing & Generation
save_every: 500              # Save checkpoint every N steps
gen_every: 1000              # Generate sample audio every N steps

# Training length
num_train_epochs: 2
max_steps: -1                # Set to positive number to override epochs
